# src/core/optimizer.py

import optuna
import pandas as pd
import numpy as np
import json
import os
import gc
import joblib
from collections import defaultdict

# Resolu√ß√£o de Path
import sys
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.data_manager import DataManager
from src.core.model_trainer import ModelTrainer
from src.logger import logger
from src.config_manager import settings

class WalkForwardOptimizer:
    """
    Orquestra a otimiza√ß√£o Walk-Forward para um TIME de modelos especialistas.
    """
    def __init__(self, full_data: pd.DataFrame):
        self.full_data = full_data
        self.trainer = ModelTrainer()
        self.n_trials = settings.optimizer.n_trials_for_cycle
        self.optimization_summary = {}

        # Defini√ß√£o das personalidades dos especialistas
        self.specialist_definitions = {
            "price_action": [
                'atr', 'bb_width', 'bb_pband', 'price_change_1m', 'price_change_5m', 
                'momentum_10m', 'volatility_ratio'
            ],
            "quant_classic": [
                'rsi', 'macd_diff', 'stoch_osc', 'adx', 'adx_power', 'cci', 'williams_r',
                'cvd', 'cvd_short_term'
            ],
            "macro_hedger": [
                'dxy_close_change', 'vix_close_change', 'gold_close_change',
                'tnx_close_change', 'btc_dxy_corr_30d', 'btc_vix_corr_30d'
            ]
        }

    def _objective(self, trial: optuna.trial.Trial, data_for_objective: pd.DataFrame, specialist_features: list) -> float:
        """A fun√ß√£o objetivo que o Optuna tentar√° maximizar."""
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'verbosity': -1,
            'boosting_type': 'gbdt',
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, log=True),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 20, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
        }
        
        score = self.trainer.train_and_backtest_for_optimization(
            data=data_for_objective,
            params=params,
            feature_names=specialist_features
        )
        return score

    def run_optimization_for_situation(self, situation_name: str, data: pd.DataFrame) -> None:
        """
        Executa a otimiza√ß√£o e SALVA os melhores modelos para cada especialista.
        """
        logger.info(f"\n{'='*20} Iniciando otimiza√ß√£o para: {situation_name.upper()} {'='*20}")
        
        situation_models_path = os.path.join(settings.data_paths.data_dir, situation_name)
        os.makedirs(situation_models_path, exist_ok=True)

        for specialist_name, specialist_features in self.specialist_definitions.items():
            logger.info(f"--- Otimizando especialista: '{specialist_name}' ---")

            study = optuna.create_study(direction="maximize")
            objective_func = lambda trial: self._objective(trial, data, specialist_features)
            study.optimize(objective_func, n_trials=self.n_trials)

            try:
                best_trial = study.best_trial
                best_score = best_trial.value
                best_params = best_trial.params
                logger.info(f"üèÜ Otimiza√ß√£o de '{specialist_name}' conclu√≠da. Melhor Score: {best_score:.4f}")

                if best_score > settings.optimizer.quality_threshold:
                    logger.info(f"   -> Score excelente! Treinando e salvando modelo final...")
                    
                    # Treina o modelo final com os melhores par√¢metros e TODOS os dados dispon√≠veis
                    final_model, final_scaler = self.trainer.train(data, best_params, specialist_features)
                    
                    if final_model and final_scaler:
                        # Salva os artefactos
                        joblib.dump(final_model, os.path.join(situation_models_path, f"model_{specialist_name}.joblib"))
                        joblib.dump(final_scaler, os.path.join(situation_models_path, f"scaler_{specialist_name}.joblib"))
                        
                        # Salva os par√¢metros e features usadas
                        artefact_info = {
                            "best_params": best_params,
                            "best_score": best_score,
                            "feature_names": specialist_features,
                            "trained_at": datetime.datetime.now().isoformat()
                        }
                        with open(os.path.join(situation_models_path, f"params_{specialist_name}.json"), 'w') as f:
                            json.dump(artefact_info, f, indent=4)
                        
                        logger.info(f"   -> ‚úÖ Modelo, Scaler e Par√¢metros para '{specialist_name}' salvos com sucesso!")

                else:
                    logger.warning(f"   -> Score de {best_score:.4f} n√£o atingiu o limiar de qualidade ({settings.optimizer.quality_threshold}). Modelo n√£o foi salvo.")

            except ValueError:
                logger.warning(f"‚ùå Nenhum trial conclu√≠do com sucesso para o especialista '{specialist_name}'.")

    def run(self) -> None:
        """Executa o processo de otimiza√ß√£o completo."""
        logger.info("\n" + "="*80 + "\n--- üöÄ INICIANDO PROCESSO DE OTIMIZA√á√ÉO DA MENTE-COLMEIA üöÄ ---\n" + "="*80)
        optuna.logging.set_verbosity(optuna.logging.WARNING)

        self.run_optimization_for_situation("all_data", self.full_data)

        logger.info("\n" + "="*80 + "\n--- ‚úÖ PROCESSO DE OTIMIZA√á√ÉO CONCLU√çDO ‚úÖ ---\n" + "="*80)

# src/core/optimizer.py

# ... (todo o resto do seu c√≥digo, como a classe WalkForwardOptimizer, continua igual) ...

if __name__ == '__main__':
    import datetime  # Import necess√°rio para a l√≥gica de salvar artefactos

    logger.info("--- üåê INICIANDO PIPELINE DE DADOS E FEATURES üåê ---")
    
    # --- A MUDAN√áA EST√Å AQUI ---
    # 1. Instanciamos o DataManager
    data_manager = DataManager()
    
    # 2. Executamos o pipeline completo. 
    #    Esta fun√ß√£o agora √© respons√°vel por verificar, popular (bootstrap), 
    #    atualizar a base de dados e adicionar features.
    df_with_features = data_manager.run_data_pipeline(symbol='BTCUSDT', interval='1m')

    # 3. Verificamos se o pipeline retornou dados v√°lidos antes de continuar
    if df_with_features is not None and not df_with_features.empty:
        logger.info("‚úÖ Pipeline de dados conclu√≠do com sucesso. Iniciando o otimizador...")
        optimizer = WalkForwardOptimizer(full_data=df_with_features)
        optimizer.run()
    else:
        logger.error("‚ùå O pipeline de dados n√£o retornou um DataFrame v√°lido. Otimiza√ß√£o abortada.")